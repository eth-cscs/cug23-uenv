%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Squashfs artifacts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The simplest way to deploy aoftware stack is to copy them to a path on a shared file system, for example if the site-policy is to install \lst{/apps/stacks/<env-name>}, the process for building a climate software stack \lst{climate-23.3} would be:
\begin{enumerate}
    \item configure the software stack with mount point \lst{/apps/stacks/climate-23.3}, and build in \lst{/dev/shm/build/climate-23.3}
    \begin{itemize}
        \item build in memory to reduce the build time compared to building on the shared file system.
    \end{itemize}
    \item copy \lst{/dev/shm/build/climate-23.3/store/*} to the location in \lst{/apps}.
\end{enumerate}

The downsides of this approach are that
\begin{itemize}
    \item depends on file system performance -- GPFS and LUSTRE file systems are not designed for the type of file access (many small files being opend and closed).
    \item requires a large amount of storage
    \item upgrading the version of requires changing the path in all scripts etc.
    \item directory structure gets complicated
    \item if we want to let users build their own stacks, they can't build in \lst{/apps}.
\end{itemize}

Each software stack is a compressed squashfs image of a directory containing the software, Spack configuration, modules and meta-data.
The squashfs artifacts associated with a programming environment can be versioned, and mounted at the same location using command line utilities or SLURM plugins.
This significantly simplifies testing and deployment of stacks in CI pipelines compared to the current approach of building software stacks on a shared file system with subdirectories for different clusters, where providing multiple versions of a software stack requires creating a directory with a different name.

Other benefits of using a squashfs for images include:
\begin{itemize}
    \item Squashfs images are cached in memory so time to configure and build software is significantly faster than if the tools are on a shared filesystem;
    \item Multiple users on the same node can load different environments on the same mount point.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CLI Utilities}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Harmen}

Discuss the command line utility for mounting squashfs images \lst{squashfs-mount}.

\begin{itemize}
    \item squashfs-mount implementation
    \item example of how it is used
    \item link to GitHub repository
    \item RPMs are provided ready to install on Cray EX.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SLURM Integration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Simon and Jonathan}

At CSCS, we employ the SLURM job scheduler.
For a software stack to be available when a job runs, the stack must be mounted in the namespace of the process executing the submission script on the head node and a command launched on the compute nodes.
To accomplish this we developed a SLURM plugin that will mount a software stack based upon the same namespace mechanism used by the command line utilities discussed earlier.
As the mount point is not globally visible, nodes can be run multiple jobs with different stacks, either from the same user or different users.
Clean up of the mount point and software stack is therefore performed automatically once the parent process terminates.
The plugin is freely available to the public\footnote{https://github.com/eth-cscs/slurm-uenv-mount}.

We have designed the plugin to be as transparent to a user's workflow as possible.
This reduces work for users and system administrators, as the plugin and squashfs-based software stack concept can easily be integrated into existing systems and workflows.
In particular, the plugin works with squashfs-mount to detect which software stacks (and their mount points) are active.
These are taken as default values to the plugin which will make them available on the head and compute nodes.
A different stack can be specified as a command-line option to any of the SLURM submission commands (e.g., sbatch or srun).
This flexibility ensures consistency between the login environment and the execution environment, while also allowing the user to use different stacks \emph{within} a script.

The plugin is written in C++ using the SPANK API for SLURM plugins.
Root-level access is required to mount the squashfs image, which means the namespace creation and mounting code is located in the API function that SLURM runs in privileged mode.
Installation also requires system-administrator privileges.

Examples:

Running on the login node:
\lstinputlisting[language=bash]{src/plugin-examples/default.sh}

A SLURM submission script:
\lstinputlisting[language=bash]{src/plugin-examples/change-env.sh}

\begin{itemize}
    \item the slurm plugin workflow
    \item plugin implementation
    \item installing the plugin
    \begin{itemize}
        \item provide any HPE XE-system specific details and workarounds
    \end{itemize}
    \item examples of using the plugin
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CI/CD}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Theofilos and Ben}

Describe the workflow:
\begin{itemize}
    \item recipes are stored in \href{https://github.com/eth-cscs/alps-spack-stacks}{github repository}.
    \item CI/CD external service is used
    \item "build" phase
    \item "test" phase
    \item deployed to JFrog artifactory
\end{itemize}

Demonstrate pulling and running an image manually from JFrog.

The workflow is under development, with the following steps to be completed
\begin{itemize}
    \item promote images to a ``production'' repository
    \item a CLI tool for listing and pulling available images.
\end{itemize}

%%% Local Variables:
%%% TeX-master: "paper"
%%% End:
