%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SquashFS artifacts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Software stacks can be deployed by copying them to a path on a shared file system, for example if the site-policy is to install \lst{/apps/stacks/<env-name>}, the process for building a climate software stack \lst{climate-23.3} would be:
\begin{enumerate}
    \item Configure the build with \lst{stack-config} with mount point \lst{/apps/stacks/climate-23.3}, and build in \lst{/dev/shm/build/climate-23.3}.
        Note that we still build in memory to reduce the build time compared to building in place on the shared file system.
    \item Once built, copy \lst{/dev/shm/build/climate-23.3/store/*} to \lst{/apps/stacks/climate-23.3}.
\end{enumerate}

Installing software stacks in shared file systems has some downsides, namely:
\begin{itemize}
    \item The user-experience is affected by file system performance -- configuration and compilation access many small files which are not well-suited to GPFS and LUSTRE.
    \item High storage overheads -- for example, a software stack with CUDA and NVHPC SDK requires at least 30 GB uncompressed.
    \item Upgrading the version of a stack by installing it in a new path requires changing that path in all downstream user scripts and workflows.
    \item Users will combine software from different stacks, often by accident, leading to difficult to debug linking and runtime bugs.
\end{itemize}

To address these issues, CSCS deploys the software stacks as compressed SquashFS images of the directory containing the software, Spack configuration, modules and meta-data.
Squashfs is an efficient and compressed read-only file system that offers is well-suited for distributing software stacks, for the following reasons:
\begin{itemize}
    \item SquashFS supports both compression and deduplication, resulting in significantly reduced storage requirements.
          The software stack for Meteo Swiss requires 34 GB uncompressed, and 13 GB as a compressed SquashFS image.
    \item Each stack is a single compressed file that includes an entire software stack, making it easy to manage in DevOps pipelines and archives:
    \begin{itemize}
        \item each CI/CD pipeline generates a single binary artifact;
        \item artifacts associated with a programming environment can be versioned and archived;
        \item new stacks can be tested transparently by mounting a pre-release stack at a common mount point.
    \end{itemize}
    \item Users can mount stacks on command using command line utilities or SLURM plugins.
        Only one stack is mounted at a time -- reducing confusion about which software is being used in a workflow.
        Multiple users on the same login or compute nodes can mount different software stacks without side-effects on other user's environments.
    \item Consistent and reproducable performance for workloads that access many small files by virtue of the whole stack being stored in a single file and file-system caching.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CLI Utilities}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Non-privileged users are able to mount SquashFS images at runtime using the \lst{squashfs-mount} command line utility, which is a small \lst{setuid} executable that creates a new mount namespace, mounts the SquashFS file through \lst{libmount}, drops privileges and executes a given command.
This procedure is very similar to SquashFS-based HPC container runtimes such as Apptainer and Sarus.

For example,

\lstinputlisting[language=bash]{src/squashfs-mount.sh}

starts a bash shell in which \lst{image.squashfs} is mounted at \lst{/user-environment}.
Thanks to mount namespaces, the mount is not visible to other processes or users.

The utility is open source, \href{https://github.com/eth-cscs/squashfs-mount}{available on GitHub} and includes RPMs for installation on Cray EX.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SLURM Integration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Simon and Jonathan}

For a software stack to be available when a job runs, the stack must be mounted in the namespace of the process executing the submission script on the head node and a command launched on the compute nodes.
To accomplish this we developed a SLURM plugin that mounts a software stack based upon the same namespace mechanism used by the command line utilities discussed earlier.
As the mount point is not globally visible, nodes can be run multiple jobs with different stacks, either from the same user or different users.
Clean up of the mount point and software stack is therefore performed automatically once the parent process terminates.
The plugin is freely available to the public\footnote{https://github.com/eth-cscs/slurm-uenv-mount}.

The plugin is designed to be as transparent to a user's workflow as possible.
This reduces work for users and system administrators, as the plugin and SquashFS-based software stack concept can easily be integrated into existing systems and workflows.
In particular, the plugin works with \lst{squashfs-mount} to detect which software stacks (and their mount points) are active.
These are taken as default values to the plugin which will make them available on the head and compute nodes.
A different stack can be specified as a command-line option to any of the SLURM submission commands (e.g., sbatch or srun).
This flexibility ensures consistency between the login environment and the execution environment, while also allowing the user to use different stacks \emph{within} a script.

The plugin is written in C++ using the SPANK API for SLURM plugins.
Root-level access is required to mount the SquashFS image, which means the namespace creation and mounting code is located in the API function that SLURM runs in privileged mode.
Installation also requires system-administrator privileges.
Typically, this means adding the path of the plugin library directly to the SLURM plugin configuration file \lst{/etc/slurm/plugstack.conf} or in a plugin-specific configuration file under \lst{/etc/slurm/plugstack.conf.d/}.

In the following example the user is on the login node, mounts a user environment using \lst{squashfs-mount}, and starts a \lst{bash} shell. 
This environment provides \lst{gcc} under \lst{/user-environments/bin}.
The user can access this version of \lst{gcc} when running on a compute node via \lst{srun} because the SLURM plugin will recognize the mounted user environment and provide it on the compute node as well.

\lstinputlisting[language=bash]{src/plugin-examples/default.sh}

The next example demonstrates using different environments throughout a SLURM submission script.
When the script executes the plugin will have already mounted the user environment (debuggers.sqfs) mounted at the time the job was submitted.
The first command runs a bash script with a different user environment (compilers.sqfs) mounted to compile a test program.
This test program is run through the debugger found in the original user environment.
The subsequent commands show an alternative procedure where the compiler environment is specified directly to srun to compile the test program.
Then the debugger is again run, this time via srun.

\lstinputlisting[language=bash]{src/plugin-examples/change-env.sh}

\todo{cover the following}
\begin{itemize}
    \item installing the plugin
    \item provide any HPE XE-system specific details and workarounds
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CI/CD}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A CI/CD pipeline is under development for the Spack-stacks provided by CSCS to the users of vClusters on Alps.
Each software stack is maintained as a \stackinator recipe in a public GitHub repository\footnote{\url{https://github.com/eth-cscs/alps-spack-stacks}}.
The recipes are publicly available, so that user-groups can use them as the basis for building their own software stacks.

CSCS has a CI/CD service, that can be configured to respond to web hooks from a GitHub repository\footnote{\url{https://gitlab.com/cscs-ci/ci-testing/containerised_ci_doc}}.
When an authorized user requests that the pipeline for a recipe be run from pull request (PR) on the GitHub recipe repository, a webhook alerts the CSCS CI/CD service.
Each CI/CD job provides a tuple {\sf (Node-type, Cluster)}, where node-type is currently one of A100, Mi200, Zen2 or Zen3.
The service launches a build task on the target node type, using the configuration for the target cluster (that configures Spack to use the system compiler, libfabric, Slurm, etc).

The \emph{build stage} performs the following steps:
\begin{itemize}
    \item Download and install \stackinator.
    \item Configure the recipe to use a common build-cache for all CI/CD pipelines.
    \item Run \stackinator \lst{stack-config} to configure a build path in \lst{/dev/shm}.
    \item Build the stack in parallel.
    \item Push the squashfs image to a job-specific path in the CSCS JFrog\footnote{\url{https://jfrog.com/artifactory}} artifactory.
\end{itemize}

Development of the CI/CD pipeline for images is on-going, with the following stages under development for release in Q3 2023:
\begin{itemize}
    \item \emph{test stage} use ReFrame to launch tests for correctness and performance.
    \item \emph{deploy stage}: deploy the image to a production artifactory where it can be accessed by users with a simple CLI interface.
\end{itemize}

%%% Local Variables:
%%% TeX-master: "paper"
%%% End:
