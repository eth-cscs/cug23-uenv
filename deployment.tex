%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Squashfs artifacts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Software stacks can be deployed by copying them to a path on a shared file system, for example if the site-policy is to install \lst{/apps/stacks/<env-name>}, the process for building a climate software stack \lst{climate-23.3} would be:
\begin{enumerate}
    \item Configure the build with \lst{stack-config} with mount point \lst{/apps/stacks/climate-23.3}, and build in \lst{/dev/shm/build/climate-23.3}.
        Note that we still build in memory to reduce the build time compared to building in place on the shared file system.
    \item Once built, copy \lst{/dev/shm/build/climate-23.3/store/*} to \lst{/apps/stacks/climate-23.3}.
\end{enumerate}

There are some downsides to the approach of installing software stacks in shared file systems, namely:
\begin{itemize}
    \item The user-experience is affected by file system performance -- configuration and compilation access many small files which are not well-suited to GPFS and LUSTRE.
    \item High memory overheads - a software stack with CUDA and NVHPC SDK can use between 30-40 GB of storage.
    \item Upgrading the version of requires changing the path in all scripts etc.
    \item if we want to let users build their own stacks, they can't build in \lst{/apps}.
\end{itemize}

Each software stack is a compressed squashfs image of a directory containing the software, Spack configuration, modules and meta-data.
The squashfs artifacts associated with a programming environment can be versioned, and mounted at the same location using command line utilities or SLURM plugins.
This significantly simplifies testing and deployment of stacks in CI pipelines compared to the current approach of building software stacks on a shared file system with subdirectories for different clusters, where providing multiple versions of a software stack requires creating a directory with a different name.

Other benefits of using a squashfs for images include:
\begin{itemize}
    \item Squashfs images are cached in memory so time to configure and build software is significantly faster than if the tools are on a shared filesystem;
    \item Multiple users on the same node can load different environments on the same mount point.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CLI Utilities}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Harmen}

Non-privileged users are able to mount squashfs images at runtime using the \lst{squashfs-mount} command line utility, which is a small \lst{setuid} executable that creates a new mount namespace, mounts the squashfs file through \lst{libmount}, drops privileges and executes a given command.
This procedure is very similar to squashfs-based HPC container runtimes such as Apptainer and Sarus.

For example,

\lstinputlisting[language=bash]{src/squashfs-mount.sh}

starts a bash shell in which \lst{image.squashfs} is mounted at \lst{/user-environment}.
Thanks to mount namespaces, the mount is not visible to other processes or users.

The utility is open source, \href{https://github.com/eth-cscs/squashfs-mount}{available on GitHub} and includes RPMs for installation on Cray EX.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SLURM Integration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Simon and Jonathan}

For a software stack to be available when a job runs, the stack must be mounted in the namespace of the process executing the submission script on the head node and a command launched on the compute nodes.
To accomplish this we developed a SLURM plugin that will mount a software stack based upon the same namespace mechanism used by the command line utilities discussed earlier.
As the mount point is not globally visible, nodes can be run multiple jobs with different stacks, either from the same user or different users.
Clean up of the mount point and software stack is therefore performed automatically once the parent process terminates.
The plugin is freely available to the public\footnote{https://github.com/eth-cscs/slurm-uenv-mount}.

We have designed the plugin to be as transparent to a user's workflow as possible.
This reduces work for users and system administrators, as the plugin and squashfs-based software stack concept can easily be integrated into existing systems and workflows.
In particular, the plugin works with squashfs-mount to detect which software stacks (and their mount points) are active.
These are taken as default values to the plugin which will make them available on the head and compute nodes.
A different stack can be specified as a command-line option to any of the SLURM submission commands (e.g., sbatch or srun).
This flexibility ensures consistency between the login environment and the execution environment, while also allowing the user to use different stacks \emph{within} a script.

The plugin is written in C++ using the SPANK API for SLURM plugins.
Root-level access is required to mount the squashfs image, which means the namespace creation and mounting code is located in the API function that SLURM runs in privileged mode.
Installation also requires system-administrator privileges.

Examples:

Running on the login node:
\lstinputlisting[language=bash]{src/plugin-examples/default.sh}

A SLURM submission script:
\lstinputlisting[language=bash]{src/plugin-examples/change-env.sh}

\todo{cover the following}
\begin{itemize}
    \item installing the plugin
    \item provide any HPE XE-system specific details and workarounds
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CI/CD}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Theofilos and Ben}

Describe the workflow:
\begin{itemize}
    \item recipes are stored in \href{https://github.com/eth-cscs/alps-spack-stacks}{github repository}.
    \item CI/CD external service is used
    \item "build" phase
    \item "test" phase
    \item deployed to JFrog artifactory
\end{itemize}

Demonstrate pulling and running an image manually from JFrog.

The workflow is under development, with the following steps to be completed
\begin{itemize}
    \item promote images to a ``production'' repository
    \item a CLI tool for listing and pulling available images.
\end{itemize}

%%% Local Variables:
%%% TeX-master: "paper"
%%% End:
