This section presents benchmarks and tests that demonstrate 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deployment productivity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Ben}

Benchmark the time to build and deploy a stack from scratch vs. with a build cache.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Developer productivity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Ben}

Squashfs deployment reduces configuration and compilation times compared to serving the same stack on a shared filesystem;

Benchmark configuration time and build time with the same stack stored in different locations:
\begin{itemize}
    \item mounted with squashfs-run
    \item in memory, i.e. \lstinline|/dev/shm|
    \item on scratch.
\end{itemize}

For the following:
\begin{itemize}
    \item simple hello world
    \item medium complexity application (Arbor)
    \item complex application, e.g. DLA-F
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overheads}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{Ben}

Quantify the memory overheads and affect on job startup time of mounting squashfs images on compute nodes at the start of SLURM jobs;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Benchmarks and Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{JG (SPH-EXA), Antonk}

Micro-benchmarks and application benchmarks that demonstrate equivalent performance to applications compiled with CPE on the same system.

\noindent\textbf{MicroBenchmark: OSU}

\todo{Ben and Theo}

\noindent\textbf{Application Benchmark: SPH-EXA}

The SPH-EXA\footnote{\url{https://github.com/unibas-dmi-hpc/SPH-EXA}} project is a multidisciplinary effort that looks to scale the Smoothed Particle Hydrodynamics (SPH) method to enable exascale hydrodynamics simulations for the fields of Cosmology and Astrophysics. 
Figure \ref{fig:gpu-A100-best} shows the performance obtained for the MPI+OpenMP+CUDA and MPI+OpenMP+HIP codes executing the Sedov--Taylor\footnote{\url{https://doi.org/10.48550/arxiv.2202.02840}} blast wave explosion test case with $400^3$ particles per gpu and $40$ time-steps.
The results show that the squashfs-based executables delivers competitive performance with that of the cpe based CPU executables on both NVIDIA A100 and AMD MI200 gpus.

\begin{figure*}[htp!]
    \input{images/sph-nvidia.tex}
    \hfill
    \input{images/sph-amd.tex}

    \caption{hello world}
    \label{fig:gpu-MI200-best}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{JG}

\begin{itemize}
    \item demonstrate DDT
    \item demonstrate a profiler?
\end{itemize}

\subsubsection{Performance tools}

\begin{table}[htp!]
    \centering
    \pgfplotstabletypeset[
        precision=0,
        every head row/.style={before row=\toprule,after row=\midrule},
        col sep=space, columns={num-tasks, H2D-MB-max, D2H-MB-max, D2D-MB-max},
        columns/num-tasks/.style={column name=A100},
        columns/H2D-MB-max/.style={column name=H2D},
        columns/D2H-MB-max/.style={column name=D2H},
        columns/D2D-MB-max/.style={column name=D2D},
    ]
        {./data/sphexa/tbl/run-report-A100-64M-SQFS-cpe2302-NSYS.tbl}
    \caption{hello world}
    \label{table:nsys-A100}
\end{table}

Table \ref{table:nsys-A100} shows the amount of CUDA memory copies (in MB) reported by NVIDIA performance tool (Nsight Systems) for the Sedov--Taylor test case.
The transfer sizes (in MB) for Host to Device (H2D), Device to Host (D2H) and Device to Device (D2D) for simulations with uenv and without uenv (cpe only) are equal, demonstrating that the performance tool can be used in both scenarios.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We will discuss collaboration with HPE to provide Cray-MPICH and other HPE software packages via spack stacks, and how we plan to deliver software for the large Grace-Hopper scale out in the second half of 2023 at CSCS.

%%% Local Variables:
%%% TeX-master: "paper"
%%% End:
