%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Efficient Deployment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This benchmark tests the impact of optimisations to reduce build time of spack stacks described in~\sect{sec:faster-builds}, namely building in memory and caching previously-built packages.

For this demonstration, we built a software stack that has all of the dependencies required to develop Arbor~\cite{paper:arbor2019,software:arbor}, a Neuroscience application written in C++ and Python.
Arbor has a extensive list of dependencies, including C++ libraries, Python and Python packages.

We time the time taken to run make on a clean build, which includes the time taken to bootstrap Spack, concretise and build all of the packages and generate the compressed SquashFS image in four different scenarios:
\begin{itemize}
    \item \textbf{scratch}: Build on an HPE Cray ClusterStor E1000 Scratch file system.
    \item \textbf{memory}: Build in \lst{/dev/shm}, i.e. \emph{in memory}.
    \item \textbf{cache}: Build in \lst{/dev/shm} using a Spack build cache that has all of the packages 
    \item \textbf{partial}: Build in \lst{/dev/shm} using a Spack build cache where the version of Python in the recipe is changed to a version that is not in the cache.
\end{itemize}
Scenario 2 quantifies the effect of building in memory, and scenarios 3 and 4 illustrate the additional benefits of using build caches.

\begin{figure}[htp!]
    \begin{center}
        \input{images/image-build.tex}
    \end{center}
    \caption{The effect of building in memory and using Spack build caches on the time to build a complete Spack stack.}
    \label{fig:image-build}
\end{figure}

\fig{fig:image-build} shows that building the image on Scratch takes 45 minutes, which is reduced to 26 minutes when building in memory -- a significant 1.7$\times$ reduction in build time.
Less than 3 minutes are required when all packages are available in a build cache, and less than 10 minutes to build the full stack when the version of Python in the recipe changed, which required rebuilding over 30 packages, including Python, py-numpy and py-mpi4py, which are non-trivial to build.

The \emph{partial} reflects the most common scenario, because the typical CI/CD and image development process requires rebuilding an image with small changes, so that only some of the packages need to be rebuilt between runs.
Furthermore, the bootstrap and compiler toolchains are typically identical between different images -- e.g. once GCC 11.3.0 has been built for one stack, it can be reused without change in another.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Developer Productivity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now test whether using compilers and libraries installed via SquashFS has any impact on the time taken to configure applications on the command line, which has a direct impact on developer productivity.
In these tests we will use the Arbor programming environment used in the previous tests -- where the environment is installed in three different ways:
\begin{enumerate}
    \item \textbf{squashfs}: a SquashFS image mounted at \lst{/user-environment}.
    \item \textbf{scratch}: installed on the Scratch file system.
    \item \textbf{memory}: stored in \lst{/dev/shm} and mounted at \lst{/user-environment} with Bubblewrap.
\end{enumerate}

First, we look at the time required to compile a single ``hello world'' C and C++ files using the GNU compiler provided by the stack:

\noindent\texttt{hello.c}:
\lstinputlisting[language=c++]{src/hello.c}
\noindent\texttt{hello.cpp}:
\lstinputlisting[language=c++]{src/hello.cpp}

As illustrated in~\tbl{tbl:hello-world-compile}, the compilation times are within 1\% when the compiler toolchain is in memory or mounted via SquashFS, and between 4-11\% slower when the toolchain is installed on the Scratch filesystem.
\begin{table}[hp!]
    \begin{center}
        \begin{tabular}{l | l l l}
                & squashfs & scratch & memory \\
                \hline
            C   &  31.1 &  34.7 ($+ 11\%$) &  31.2 ($ < 1 \%$) \\
            C++ & 266   & 276   ($+3.8\%$) & 264   ($ < 1 \%$) \\
        \end{tabular}
    \end{center}
    \caption{The time taken (in ms) to compile simple hello world C and C++ files using the programming stack installed in different locations.}
    \label{tbl:hello-world-compile}
\end{table}
    
A more involved example is to build Arbor using the stack developed above.
This is broken into two steps:
\begin{enumerate}
    \item \textbf{configure}: run CMake to configure a build with MPI and Python enabled, and use generated build files for Ninja: \lst{CC=mpicc CXX=mpic++ cmake ../arbor -DARB_WITH_MPI=on -DARB_WITH_PYTHON=on -G Ninja}.
    \item \textbf{build}: run Ninja to build Arbor.
\end{enumerate}

To isolate the file system overheads of accessing the stack, the Arbor source code and build path are in \lst{/dev/shm}.
The results in~\tbl{tbl:arbor-compile} show that the SquashFS mount and in memory are equivalent, which there is a performance penalty of between 8-23\% on Scratch.

\begin{table}[hp!]
    \begin{center}
        \begin{tabular}{l | l l l}
                        & squashfs & scratch & memory \\
                \hline
            configure   & 2.52    & 3.09 ($+23\%$)  & 2.53 ($<1\%$) \\
            build       & 33.9    & 36.7 ($+8\%$)   & 33.8 ($<1\%$) \\
        \end{tabular}
    \end{center}
    \caption{The time taken (in s) to configure and build Arbor.}
    \label{tbl:arbor-compile}
\end{table}


We note the tests in this section were run when the file system was not in heavy use, and smaller differences were observed when tested on a flash-based Lustre store, so the performance gains of using SquashFS are modest.
However, in our experience performance of workloads that access many files in a SquashFS stack is very consistent, regardless of load on the system, and when the SquashFS image itself is stored in Scratch.
On the other hand, compilation and configuration times vary greatly for software stacks installed on Lustre or GPFS filesystems -- when the file system is under heavy load compilation can be much slower.
As such, SquashFS is both faster and more consistent and predictable than installing software on shared file systems, improving the quality of the user experience on our systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Benchmarks and Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we present benchmarks compiled with CPE and \spack stacks on the same system, everything else being equal.
The purpose of the benchmarks is not to compare the performance of node types, or evaluate the efficiency of the benchmarks, instead the objective is to demonstrate equivalent performance of the benchmarks when built using the CPE software stack and spack-stacks.
All of the benchmarks use \craympich, in order to understand whether repackaging \craympich for installation with \spack has any impact on performance.

\noindent\textbf{MicroBenchmark: OSU}

Selected OSU microbenchmarks\footnote{\url{https://mvapich.cse.ohio-state.edu/benchmarks/}} were run on the vCluster Clariden, which has nodes with 4 NVIDIA  A100 GPUs, a single socket AMD Zen3 Milan CPU, and 4 Slingshot 11 NICs.
The following three selected benchmarks run in both host-host and device-device configurations:
\begin{itemize}
    \item \textbf{osu\_bw}: Point to Point bandwidth test. The benchmark was run between two ranks on different nodes.
    \item \textbf{osu\_latency}: Point to Point latency test. The benchmark was run between two ranks on different nodes.
    \item \textbf{osu\_alltoall}: All to all latency test. The benchmark was run between 16 ranks on 4 nodes, with one GPU per rank when running .
\end{itemize}

\begin{figure*}[htp!]
    \begin{center}
        \input{images/osu-p2p-bw-gpu.tex}
        \hfill
        \input{images/osu-p2p-bw-cpu.tex}
        \newline
        \input{images/osu-p2p-lat-gpu.tex}
        \hfill
        \input{images/osu-p2p-lat-cpu.tex}
        \newline
        \input{images/osu-a2a-lat-gpu.tex}
        \hfill
        \input{images/osu-a2a-lat-cpu.tex}
    \end{center}
    \caption{OSU benchmark results comparing cray-mpich performance when built using CPE and spack-stacks (uenv) for host-host (cpu) and device-device (gpu) configurations.}
    \label{fig:osu}
\end{figure*}

The following wrapper script was used to launch all of the jobs (note that the GPU flags will have no impact on the CPU-only runs).
\lstinputlisting[language=bash]{src/osu-launch.sh}
The wrapper script ensures optimal affinity of GPUs with NUMA regions, and we let \craympich select the NIC in each case -- when using both CPE and Spack stacks the same NIC was assigned.

The versions of compilers and tools did not match exactly:
\begin{center}
    \begin{tabular}{l |c  c }
                      & CPE   & Spack Stack \\
          \hline
        osu-benchmark & 5.9   & 5.9       \\
        cray-mpich    & 8.1.21& 8.1.24    \\
        gcc           & 11.2  & 11.3      \\
        cuda          & 11.6  & 11.8      \\
    \end{tabular}
\end{center}

The most recent version of CPE installed on the system was v22.12, and the stack was built using the version of cray-mpich in v23.3. However, earlier benchmarks and tests using other versions of cray-mpich from CPE and \spack stacks gave the same results.

The OSU benchmark results, plotted in \fig{fig:osu}, illustrate that there is no discernable benefit either way of using cray-mpich from CPE or installed via Spack.

\noindent\textbf{Application Benchmark: SPH-EXA}

The SPH-EXA\footnote{\url{https://github.com/unibas-dmi-hpc/SPH-EXA}} project is a multidisciplinary effort that looks to scale the Smoothed Particle Hydrodynamics (SPH) method to enable exascale hydrodynamics simulations for the fields of Cosmology and Astrophysics. 
This section focuses on comparing the behavior of the code built with and without the stackinator tool.
For this, we constructed two base images: one for NVIDIA GPUs and one for AMD GPUs.
The compiler and libraries included in these images were based on cray-mpich/8.1.21, in addition to gcc/11.x and either cuda/11.8 or hip/5.2.
Next, we mounted the images to access the compiler and libraries for building our MPI+OpenMP+CUDA and MPI+OpenMP+HIP versions of the code. Finally, we executed the executables and compared the results with those of the same code built with the Cray Programming Environment.

\begin{figure}[htp!]
    \begin{center}
        \input{images/sph-nvidia-nodes.tex}
        \input{images/sph-amd-nodes.tex}
        \input{images/sph-amdcpu-nodes.tex}
    \end{center}
    \caption{Weak scaling results on A100 and Mi250x GPU and AMD CPU nodes for SPH-EXA (higher is better).}
    \label{fig:sph-weak}
\end{figure}

\fig{fig:sph-weak} shows the performance obtained for the codes executing the Sedov--Taylor\footnote{\url{https://doi.org/10.48550/arxiv.2202.02840}} blast wave explosion test case with $400^3$ particles per gpu and $40$ time-steps, and for the MPI+OpenMP CPU-only version of the code with $483^3$ particles per compute node.

The results show that the squashfs-based executables deliver competitive performance with that of the CPE based executables on both GPU architectures and multicore.

\noindent\textbf{Application Benchmark: GROMACS}

\begin{figure}[htp!]
    \begin{center}
        \input{images/gromacs.tex}
    \end{center}
    \caption{GROMACS strong scaling measured in ns/day when built using spack stacks and CPE.}
    \label{fig:gromacs-strong}
\end{figure}

The GROMACS strong scaling results, plotted in \fig{fig:gromacs-strong}, show very.
The results with CPE were between 1\%-1.5\% faster than the Spack-stack.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\assign{JG}

\begin{itemize}
    \item demonstrate DDT
    \item demonstrate a profiler?
\end{itemize}

\subsubsection{Performance tools}

\begin{table}[htp!]
    \centering
    \pgfplotstabletypeset[
        precision=0,
        every head row/.style={before row=\toprule,after row=\midrule},
        col sep=space, columns={num-tasks, H2D-MB-max, D2H-MB-max, D2D-MB-max},
        columns/num-tasks/.style={column name=A100},
        columns/H2D-MB-max/.style={column name=H2D},
        columns/D2H-MB-max/.style={column name=D2H},
        columns/D2D-MB-max/.style={column name=D2D},
    ]
        {./data/sphexa/gpu/run-report-A100-64M-SQFS-cpe2302-NSYS.tbl}
    \caption{CUDA memcpy}
    \label{table:nsys-A100}
\end{table}

Table \ref{table:nsys-A100} shows the amount of CUDA memory copies (in MB) for the Sedov--Taylor test case.
The performance data was collected with the NVIDIA {Nsight Systems\footnote{\url{https://developer.nvidia.com/nsight-systems}}}  tool.
The transfer sizes (in MB) for Host to Device (H2D), Device to Host (D2H) and Device to Device (D2D) for simulations with uenv and without uenv (cpe only) are equal, demonstrating that the performance tool can be used in both scenarios.

\begin{figure}[htp!]
    \begin{center}
        \input{images/sph-nvidia-nodes-scorep.tex}
    \end{center}
    \caption{Weak scaling profiling results on AMD EPYC 7A53 CPU nodes for SPH-EXA.}
    \label{fig:sph-weak-scorep}
\end{figure}

\fig{fig:sph-weak-scorep} shows profiling results for the Sedov--Taylor test case on CPU nodes.
The performance data was collected with the Score-P\footnote{\url{https://score-p.org}} tool (version 8.1).
The breakdown of the runtime into different regions such as USER, OpenMP and MPI demonstrates that the performance tool can be used in both environments.

% keep for reference: mpip
% \begin{table}[htp!]
%     \centering
%     \pgfplotstabletypeset[
%         precision=2,
%         every head row/.style={before row=\toprule,after row=\midrule},
%         col sep=space, columns={cn, pctmpi_cpe1, pctmpi_cpe2, pctmpi_uenv1, pctmpi_uenv2},
%         columns/pctmpi_cpe1/.style={column name=$\% CPE_{try1}$},
%         columns/pctmpi_cpe2/.style={column name=$\% CPE_{try2}$},
%         columns/pctmpi_uenv1/.style={column name=$\% UENV_{try1}$},
%         columns/pctmpi_uenv2/.style={column name=$\% UENV_{try2}$},
%     ]
%         {./data/sphexa/mpi/amdepyc_7a53-mpi.tbl}
%     \caption{MPI $\%$, AMD 7A53 cpu}
%     \label{table:mpi}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We will discuss collaboration with HPE to provide Cray-MPICH and other HPE software packages via spack stacks, and how we plan to deliver software for the large Grace-Hopper scale out in the second half of 2023 at CSCS.

%%% Local Variables:
%%% TeX-master: "paper"
%%% End:
