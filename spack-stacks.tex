\assign{Ben}

We developed a tool To build programming environments from a descriptive YAML recipe of the compilers and software packages in the environment, generates a set of Spack~\cite{gamblin:sc15} environment descriptions, and builds the environment using Makefiles.
Reproducable builds are achieved by fixing the versions of both Spack and the software packages, and rolling releases that provide the most up to date packages and configurations can be created by targeting a branch of Spack (such as the develop branch), and letting Spack pick the package versions.

The tool, named \href{https://github.com/eth-cscs/stackinator}{\stackinator}, is a Python application that takes generates a set of Spack environments, and Makefiles to build them from a recipe.
Environments are built in the following high level workflow:
\begin{enumerate}
    \item Build a user-specified version of gcc as a bootstrap compiler.
    \item Build the compiler toolchains (currently GCC, NVHPC and LLVM are supported):
    \begin{itemize}
        \item each compiler toolchain is built in a separate environment, using the bootstrap compiler
    \end{itemize}
    \item Package environments: build "programming environments", each with a unique:
    \begin{itemize}
        \item Compiler, MPI configuration and GPU configuration
        \item List of packages with their Spack specs.
    \end{itemize}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MPI}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Open source MPI distributions -- OpenMPI, MVAPICH3 and MPICH -- are actively developing support for Slingshot 11 with libfabric.
However, at the time of writing the only MPI with robust Slingshot 11 support is the \craympich bundled with the CPE, for which source code is not available.

One of the objectives of this work is to provide software stacks without installing CPE.
In order to provide \craympich, we develop a process for repackaging the compiler wrappers, library, headers and dependencies such as PMI that can be installed as a spack binary package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 1: extract and repackage RPMs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Expand the following RPMs from the CPE download:

\begin{enumerate}
    \item \lstinline{cray-mpich-8.1.18-gnu91-0-4.sles15sp3.x86_64.rpm}
    \begin{itemize}
        \item The \
        \item For the NVIDIA HPC SDK use \lstinline{cray-mpich-8.1.18-nvidia207-0-4.sles15sp3.x86_64.rpm}.
    \end{itemize}
    \item \lstinline{cray-mpich-8.1.18-gtl-0-4.sles15sp3.x86_64.rpm}
    \begin{itemize}
        \item The GTL (GPU Transport Layer) libraries that implement GPU-aware communication for NVIDA and AMD GPUs.
    \end{itemize}
    \item \lstinline{cray-pmi-[version].rpm}
\end{enumerate}

We can't provide a single binary distribution for all compilers, because the Fortran modules in the \lstinline{include} path are compiler specific.

Extract the following files:

\input{src/cray-mpich-tree-nosym.txt}

Note that for brevity, the symlinked library files were removed above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 2: patch MPI wrappers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As opposed to the CPE, which provides the \lstinline{CC}, \lstinline{cc} and \lstinline{ftn} binary compiler wrappers, our package uses the MPI compiler wrapper scripts that are installed in locations like \lstinline{/opt/cray/pe/mpich/8.1.21/ofi/gnu/9.1/bin}.

By default, these wrappers use environment variables set by CPE modules to select the compiler and link architecture-specific libraries.
Each of the wrappers is parameterised on three parameters \lstinline{@CC@}, \lstinline{@@PREFIX@@} and \lstinline{@@GTL_LIBRARY@@}, which are later set by spack to:
\begin{itemize}
    \item hard code the full path to the wrapped compiler;
    \item set paths \lstinline{prefix}, \lstinline{includedir}, to the cray-mpich \spack installation path;
    \item explicitly link \lstinline{-lmpi_gtl_cuda} \lstinline{-lmpi_gtl_hsa} when \craympich is built with the \lstinline{+cuda} or \lstinline{+rocm} variants respectively.
    \begin{itemize}
        \item this fixes the common runtime error ``MPIDI\_CRAY\_init: GPU\_SUPPORT\_ENABLED is requested, but GTL library is not linked''.
    \end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 3: create patch binary spack package}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The 
The tar balls are stored in a \href{https://jfrog.com/artifactory/}{JFrog Artifactory} -- a self-hosted artifact store accessible only on the CSCS network.
The \spack package directly downloads from the artifactory, so it can only be run on CSCS systems.

\href{https://github.com/eth-cscs/stackinator/blob/master/stackinator/repo/packages/cray-mpich/package.py}{\lstinline{/stackinator/repo/packages/cray-mpich/package.py}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimizing build times}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Using Spack to build a full software stack -- with multiple compilers, libraries and tools -- is time and resource consuming.
A simple stack based on GCC that provides Python, \craympich and cuda, will take in the order of half an hour to build, and environments for the ROCM GPU stack take hours to build from scratch on a 64-core Epyc CPU.

It is important to reduce build times where possible, so that maintainers of stacks can iterate and test combinations of packages, and for timely execution of CI/CD pipelines for deploying stacks.

\textbf{Parallelise the build}

A spack stack requires building many packages, 

The build is parallelised using multiple methods:
\begin{itemize}
    \item the stackinator tool generates makefiles that are called recursively, with the packages in each concretised environment built using \spack-generated Makefiles so that packages that don't depend on one-another to be built concurrently.
    \item a single jobserver is used to distribute the package builds over the available cpu cores.
\begin{end}

\textbf{Perform builds in memory}

The \stackinator tool supports building software stacks for installation at mount points where the build process does not have write permissions, e.g. the default \lstinline{/user-environment} path is read-only.
The tool uses \href{https://github.com/containers/bubblewrap}{bubblwrap} to mount the build path .
This can be used to optimise build times by creating the build path in memory, for example in \lstinline{/dev/shm/${USER}/${PROJECTNAME}}, so that all of the dependencies are built and stored in memory.

\textbf{Reuse previously built packages with build-caches}

\begin{itemize}
    \item Builds can be performed "in memory"
    \begin{itemize}
        \item benchmark results comparing scratch to "in memory"
    \end{itemize}
    \item Optional configurations can be provided for Spack build caches to reduce build times.
    \begin{itemize}
        \item benchmark results comparing rebuild no cache, rebuild with cache, rebuild changed spec (e.g. MPI) from cache.
    \end{itemize}
\end{itemize}
The programming environment can be used as Spack upstream for users, and the tool also allows fine-grained control over module file generation, to provide an optional module environment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Example Stack Configuration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section an example recipe for the application Arbor~\cite{paper:arbor2019,software:arbor}, a neuroscience application.

\hilight{Theo and I are discussing producing a simpler environment that builds some microbenchmarks for different arch, which we can also use for the benchmarks section.}

\lstinputlisting[language=yaml]{src/arbor-recipe/config.yaml}
\lstinputlisting[language=yaml]{src/arbor-recipe/compilers.yaml}
%\lstinputlisting[language=yaml]{src/arbor-recipe/environments.yaml}

