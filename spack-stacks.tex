This section introduces a workflow and tooling for building use-case-specific programming environment (PE) stacks on top of a base node image with CrayOS and core dependencies such as libfabric and Slurm, that does not require the CPE.

The main tool is \href{https://github.com/eth-cscs/stackinator}{\stackinator}\footnote{\url{https://github.com/eth-cscs/stackinator}}, which uses \spack~\cite{gamblin:sc15} to build a complete PE stack in a directory, which can then be deployed as SquashFS images or as directories on a shared file system.
\stackinator is an open-source Python application, that is \emph{opinionated} for targeting the vClusters on Alps\footnote{Minimal changes are required to generate stacks for other systems -- a version that generates stacks for AWS Gravitron 3 clusters was created in a hackathon.}, in the sense that it:
\begin{itemize}
    \item makes design decisions that focus on reproducability and performance tuning for the target SlingShot 11 network and node architectures available on Alps;
    \item provides limited configuration options for compilers -- the tool configures the full compiler specification according to CSCS best practices;
    \item and provides limited configuration options for MPI -- only \craympich is fully supported (with future support for MPICH and OpenMPI planned) and for which the tool will configure for Slingshot 11 and accelerator compatibility.
\end{itemize}

The following sections describe the workflow, from recipe to squashfs-images, and the novel and \crayex-specific implementation details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stack Specification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\stackinator generates stacks from a descriptive YAML file recipe.
To explain \spack recipes, we will work with an example stack for development on Cray EX EX235n nodes with NVIDIA A100 GPUs and AMD Zen3 Epyc CPUs.
The stack supports development of GPU-aware MPI applications with both GCC and CUDA, and applications with NVIDIA HPC SDK using OpenACC.
It is a stripped down version of a software stack that CSCS provides to the Swiss National Weather Service (MeteoSwiss) on Alps, specifically:
\begin{itemize}
    \item A GCC 11.3 compiler tool chain.
    \item An NVHPC 22.7 compiler tool chain.
    \item A GCC programming environment \lst{prgenv-gcc} with CUDA-aware \craympich, OSU Benchmarks, OpenBLAS and CUDA 11.8.
    \item An NVHPC programming environment \lst{prgenv-openacc} with CUDA-aware \craympich, OSU Benchmarks and CUDA 11.8.
\end{itemize}

The \stackinator recipe is composed of the following files, stored in a common path:
\begin{itemize}
    \item  \lstinline{config.yaml} specifies where the image will be installed / mounted (the CSCS default is \emph{/user-environment}), the version of \spack and optional configuration for a \spack build cache. 
        \lstinputlisting[language=yaml]{src/nv-recipe/config.yaml}
        Reproducible builds use a release branch / version tag a specific commit of \spack. A rolling release can be configured by using the \lst{develop} branch of \spack, which will build with the most recent \spack recipes.

    \item \lstinline{compilers.yaml} describes the compiler tool chains that the stack provides:
        \lstinputlisting[language=yaml]{src/nv-recipe/compilers.yaml}
        The bootstrap version of GCC built using the system compiler (GCC 7.5 at the time of writing) is not provided as a module or part of the Spack upstream presented to users, instead it is used to build the subsequent GCC compiler toolchains.
        This step is required to ensure that GCC is built using a compiler that can generate instructions optimised for the target Zen2 and Zen3 micro-architecture CPUs.
        It is also mandatory to specify at least one version of GCC, in this case GCC 11.3, which is the highest version compatible with CUDA 11.
        The LLVM tool chains are optional, with support for installing multiple versions of the NVIDIA HPC-SDK and LLVM/Clang.

    \item \lst{environment.yaml} describes the software packages:
        \lstinputlisting[language=yaml]{src/nv-recipe/environments.yaml}
        The software packages are configured as environments, each built using the compiler tool chains built previously, and configured with a (optional) single implementation of \craympich, that can optionally be configured for CUDA or ROCM support.

    \item \lst{packages.yaml} and \lst{modules.yaml} make \spack use packages installed on the system and generate modules files, respectively.
        These follow the YAML specifications for the \spack configuration files with the same names.

    \item \lst{repo} is an optional path containing a \href{https://spack.readthedocs.io/en/latest/repositories.html}{\spack repository}\footnote{\url{https://spack.readthedocs.io/en/latest/repositories.html}}, for overriding \spack's implementations or providing support for new packages.

\end{itemize}

Under the hood, the software in the stack is built in a set of \spack environments.
For the example stack, there are five environments, illustrated in~\fig{fig:env-dag}.

\begin{figure}[htp!]
    \begin{center}
    \input{images/env-dag.tex}
    \end{center}
    \caption{The dependency graph for the \spack environments that are generated internally by \stackinator to build the example \spack stack. The blue boxes are environments used to build and provide compiler toolchains, and the green boxes show the software environments that are built using the compilers. The red boxes show the compiler toolchain that is used to build the downstream packages.\newline The bootstrap compiler is built first using the system GCC 7.5, followed by the gcc tool chain, then the llvm tool chain which requires GCC 11. Finally two environments are built: prgenv-gcc with GCC 11, and prgenv-openacc which contains packages built with both GCC 11 and NVHPC 22.7.}
    \label{fig:env-dag}
\end{figure}

The Spack stack requires external packages installed on the base node image, that implement architecture-specific features and support, and can vary between vClusters and over time.
For example, the location of libfabric moves every time the installed version of libfabric is upgraded.
A set of ``cluster configurations'' for each of the vClusters on Alps is maintained separately from the recipes in the \stackinator tool:
\begin{itemize}
    \item A \spack \lst{compilers.yaml} file that specifies the default GCC compiler tool chain on the vCluster (GCC 7.5 at the time of writing), used to bootstrap the \spack stack build.
    \item A \spack \lst{packages.yaml} file that specifies externally installed software packages that Spack should never build, including:
    \begin{itemize}
        \item \lst{libfabric}: the libfabric library installed in \lst{/opt/cray/libfabric/} with CXI provider.
        \item \lst{slurm}: vClusters can have different versions of Slurm installed: at the time of writing versions 20.11.9 and 22.05.2 are used.
        \item \lst{xpmemm} and \lst{rdma-core}: required by some communication libraries.
    \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stack Configuration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\stackinator follows the familiar configure-build-install workflow used to install software.
It provides a CLI tool \emph{stack-config}, that takes a generic recipe that can be built on any (vCluster, mount point) combination, and generates a build path that contains a Makefile, \spack environment descriptions, and a copy of \spack used to build the stack.

If the recipe that describes the example environment in~\fig{fig:env-dag} is in the path \lstinline{~/recipes/nvidia}, the following \emph{stack-config} command can be used to generate a build configuration:
\begin{lstlisting}
> stack-config -r ~/recipes/nvidia \
             -b /dev/shm/nvidia-build \
             -s hohgant
\end{lstlisting}
where the generated build path is in \lstinline{/dev/shm/nvidia-build} and the build is configured for the vCluster Hohgant.
A simplified version of the generated build directory structure is illustrated in~\fig{fig:build-tree}.

\begin{figure}[htp!]
    \input{src/build-path.tex}
    \caption{Structure of the build path generated by the \stackinator tool. The files marked in blue are generated by \spack during the build process in each of the five environment paths, and are only shown in the bootstrap path for brevity.}
    \label{fig:build-tree}
\end{figure}

The environment is built using the top-level Makefile, which executes the following steps:
\begin{enumerate}
    \item (optional) Configure the build cache
    \item Call \lst{compilers/Makefile}:
    \begin{enumerate}
        \item Concretize bootstrap.
        \item Build bootstrap.
        \item Concretize GCC.
        \item Build GCC.
        \item Concretize LLVM (NVHPC).
        \item Build LLVM (NVHPC).
    \end{enumerate}
    \item Call \lst{environments/Makefile}:
    \begin{enumerate}
        \item Concretize prgenv-gcc and prgenv-openacc \emph{concurrently}
        \item Build prgenv-gcc and prgenv-openacc\emph{concurrently}
    \end{enumerate}
    \item Generate \lst{store/config}.
    \item (optional) generate \lst{store/modules}.
    \item Generate SquashFS image of \lst{store}.
\end{enumerate}

The \lstinline{compilers.yaml}, \lstinline{packages.yaml} and \lstinline{Makefile} for each of the \spack environments are generated in the Makefile using \spack. The commands used generate the respective files required to build the "prgenv-gcc" are summarised:
\begin{itemize}
    \item \lst{prgenv-gcc/compilers.yaml}:
        \begin{lstlisting}
> gcc_prefix= spack -e ../compilers/gcc \
        find --format '{prefix}' gcc@11
> spack compiler find --scope=user \
        $(compiler_bin_dirs $gcc_prefix) \end{lstlisting}
        Where the function \lst{compiler_bin_dirs} is a function that returns \lst{bin} path of the compiler, omitted for brevity.

    \item \lst{prgenv-gcc/packages.yaml}:
        \begin{lstlisting}
> spack external find --not-buildable --scope=user  perl diffutils gettext \end{lstlisting}

    \item \lst{prgenv-gcc/Makefile}:
        \begin{lstlisting}
> spack -e prgenv-gcc/ concretize -f
> spack -e prgenv-gcc/ env depfile \
        -o prgenv-gcc/Makefile \end{lstlisting}
        First, the environment is concretized (\spack generates the full set of packages and their dependencies for the environment), then a Makefile that facilitates building packages in parallel is generated.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Building a Stack}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Make is run with an empty environment with \lst{env}, to improve reproducibility by eliminating the effect of environment variables on the build:
\begin{lstlisting}
> cd /dev/shm/nvidia-build
> env --ignore-environment \
    PATH=/usr/bin:/bin:`pwd`/spack/bin \
    make store.squashfs -j32
\end{lstlisting}
Two artifacts are generated by the build:
\begin{itemize}
    \item \lst{store}: a path contains the full PE, ready to be copied to its final location.
    \item \lst{store.squashfs} path compressed in a SquashFS image, which can be mounted at the mount point.
\end{itemize}
The deployment of the SquashFS images is covered in~\sect{sec:deployment}.

The build process uses the Bubblewrap tool when running all spack commands to mount the following locations:
\begin{itemize}
    \item \lst{/dev/shm/nvidia-build/store} is mounted at the final location for the \spack stack. Thus the stack is built in a location where the user has write permissions, and allows faster in-memory builds (see~\sect{sec:faster-builds}).
    \item \lst{/dev/null} is mounted at HOME to improve reproducibility by ignoring any \spack configuration in HOME.
    \item \lst{/dev/shm/nvidia-build/tmp} is mounted at \lst{/tmp} to retain all \spack logs.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MPI}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Open source MPI distributions -- namely OpenMPI, MVAPICH2 and MPICH -- are actively developing support for Slingshot 11 with libfabric.
However, at the time of writing the only MPI with robust Slingshot 11 support is the \craympich bundled with the CPE, for which source code is not available.

One of the objectives of this work is to provide software stacks without installing CPE.
In order to provide \craympich, we develop a process for repackaging the compiler wrappers, library, headers and dependencies like PMI that can be installed as a \spack binary package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Step 1: extract and repackage RPMs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% package names for nvhpc (rpm -qi *rpm | grep Name)
% Name        : cray-mpich-8.1.24-gtl
% Name        : cray-mpich-8.1.24-nvidia207
% Name        : cray-pmi-6.1.9
% Name        : cray-pmi-devel-6.1.9

% package names for gnu
% Name        : cray-mpich-8.1.24-gnu91
% Name        : cray-mpich-8.1.24-gtl
% Name        : cray-pmi-6.1.9
% Name        : cray-pmi-devel-6.1.9

The first step is to create a single directory tree that contains only the required files by extracting them from different RPMs in the CPE distribution downloaded from HPE as illustrated in~\fig{fig:mpich-tree}.

\begin{figure}[htp!]
    \input{src/cray-mpich-tree-nosym.txt}
    \caption{The directory containing cray-mpich and its dependencies. Note that for brevity symlinked library files are removed, and wildcards are used to describe headers.}
    \label{fig:mpich-tree}
\end{figure}

For example, the cray-mpich 8.1.24 packages contains files cherry-picked from the following RPMS in the distribution in CPE 23.3:
\begin{enumerate}
    \item \lst{cray-mpich-8.1.24-gnu91} for GCC  and \lst{cray-mpich-8.1.24-nvidia207} for NVHPC.
    \item \lst{cray-mpich-8.1.18-gtl}
    \begin{itemize}
        \item The GTL (GPU Transport Layer) libraries that implement GPU-aware communication for NVIDIA and AMD GPUs.
    \end{itemize}
    \item \lst{cray-pmi-6.1.9}
    \item \lst{cray-pmi-devel-6.1.9}
\end{enumerate}
Note that the specific set of RPMs can change from release to release, so the process of extracting the files from RPMs has not been automated.
It takes a developer an hour to perform the packaging by hand for each new CPE release.

Separate GCC and NVHPC binary distributions are created because It is not possible to provide a single binary distribution of each cray-mpich version for all compilers, because the Fortran modules in the \lst{include} path are compiler specific.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Step 2: Patch MPI wrappers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As opposed to the CPE, which provides the \lst{CC}, \lst{cc} and \lst{ftn} binary compiler wrappers, the Spack package uses the MPI compiler wrapper scripts \lst{mpicc}, \lst{mpicxx} and \lst{mpifort} that are normally installed in locations like \lst{/opt/cray/pe/mpich/8.1.24/ofi/gnu/9.1/bin}.

By default, these wrappers use environment variables set by CPE modules to select the compiler and link architecture-specific libraries.
We modify each of the wrappers by parameterizing them on three parameters -- \lst{@CC@}, \lst{@@PREFIX@@} and \lst{@@GTL_LIBRARY@@} --
that are set by \spack when they are installed:
\begin{itemize}
    \item hard code the full path to the wrapped compiler;
    \item set paths \lst{prefix}, \lst{includedir}, to the cray-mpich \spack installation path;
    \item explicitly link \lst{-lmpi_gtl_cuda} \lst{-lmpi_gtl_hsa} when \craympich is built with the \lst{+cuda} or \lst{+rocm} variants respectively.
    \begin{itemize}
          \item this fixes the common runtime error ``MPIDI\_CRAY\_init: GPU\_SUPPORT\_ENABLED'' is requested, but GTL library is not linked.
    \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Step 3: Create a \spack package}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The repackaged \craympich tar balls are stored in a \href{https://jfrog.com/artifactory/}{JFrog Artifactory}\footnote{\url{https://jfrog.com/artifactory/}} -- a self-hosted artifact store accessible only on the CSCS network that can only be accessed on CSCS systems.
A custom \spack package that installs cray-mpich from the tar balls is distributed with \stackinator\footnote{See \lst{stackinator/repos} in the repository \url{https://github.com/eth-cscs/stackinator}.}, but can only be run on Alps due to this restriction.
To use this process at another site, one would have to create the tar balls, and update the \spack package accordingly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Efficient Stack Builds}
\label{sec:faster-builds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Using Spack to build a full software stack -- with multiple compilers, libraries and tools -- is time and resource consuming.
A simple stack based on GCC that provides Python, \craympich and CUDA, will take in the order of half an hour to build, and environments for the ROCM GPU stack take over two hours to build from scratch on a 64-core Epyc CPU.

It is important to reduce build times where possible, so that maintainers of stacks can iterate and test combinations of packages, and for timely execution of CI/CD pipelines for deploying stacks.
This section will document the three strategies that \stackinator uses to achieve this.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Parallelise the build}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As illustrated in~\fig{fig:env-dag}, building a \spack stack involves building a \spack environments with a DAG of dependencies dictating the order of environment concretization/installation.
In turn, installing an environment builds individual packages, which have their own DAG of dependencies.

The Makefiles generated by the \stackinator define the dependencies between environments -- facilitating the concurrent concretization and installation of independent environments. The Makefile used to build each environment is generated using the
\href{https://spack.readthedocs.io/en/latest/environments.html#generating-depfiles-from-environments}{dependency file generation}
feature of \spack, which can build multiple packages in parallel.
A single jobserver is used to parallelise the concurrent environment and package builds.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Perform builds in memory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \stackinator tool supports building software stacks for installation at mount points where the build process does not have write permissions.
For example, on Alps the default mount point is \lst{/user-environment}, which is read-only.
The software stack that is to be installed at \lst{/user-environment} is built in the \lst{store} subdirectory of the build path, where the builder has write permissions.
The built process uses
\href{https://github.com/containers/bubblewrap}{bubblewrap}\footnote{\url{https://github.com/containers/bubblewrap}}
to mount the \lst{store} path at \lst{/user-environment} for all calls to \spack.
In this way, all software is built ``in place'' and no relocation is required.

Build times can be signficantly reduced by creating the build path in memory, for example in \lst{/dev/shm/build}, so that all of the dependencies are built and stored in memory, instead of on a slower shared file system.
All of the Cray EX nodes on Alps have 512 GB of memory, which is sufficient for building software stacks, though it is important that the memory is cleaned up, preferably via an automated policy.
See \sect{sec:efficient-deployment} for a comparison between building in memory and on a shared filesystem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Cache previously built packages}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The most effective way to reduce build times is to not rebuild previously built packages.
\spack provides \href{https://spack.readthedocs.io/en/latest/binary_caches.html}{build caches}, which facilitate pushing and pulling pre-built packages to S3 buckets or a file system.
\stackinator recipes can include optional \lst{mirrors.yaml} file and a private key, to enable build caches.
Providing the location of a build cache with \lst{mirrors.yaml} will enable pulling packages from the cache, and if a key is provided \stackinator will also push all packages to the build cache as they are built.


%%% Local Variables:
%%% TeX-master: "paper"
%%% End:
