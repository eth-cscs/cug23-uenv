We developed a workflow and tooling for building use-case-specific PE stacks, on top of a base node image with CrayOS and core dependencies such as libfabric and Slurm, without CPE installed.

The main tool is \href{https://github.com/eth-cscs/stackinator}{\stackinator}, which uses \spack~\cite{gamblin:sc15} to build complete PEs in a directory, which can then be deployed as SquashFS images or as directories on a shared file system.
\stackinator is an open-source Python application, that is \emph{opinionated}\footnote{minimal changes are required to generate stacks for other systems -- a version that generates stacks for AWS Gravitron 3 clusters was created in a hackathon.}, in the sense that it:
\begin{itemize}
    \item makes design decisions that focus on reproducability and performance tuning for the target SlingShot 11 network and node architectures available on Alps;
    \item provides limited configuration options for compilers -- the tool configures the full compiler spec according to CSCS best practices;
    \item and provides limited configuration options for MPI -- only cray-mpich is supported fully, and in the future MPICH and OpenMPI will be supported, for which the tool will configure for Slingshot 11 and accelerator compatibility.
\end{itemize}

The following sections describe the workflow, from recipe to squashfs-images, and the novel and \crayex-specific implementation details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stack Specification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The discussion of the \spack recipes motivated will be motivated by an example stack for development on Cray EX EX235n nodes, with NVIDIA A100 GPUs and AMD Zen3 Epyc CPUs. The stack supports development of GPU-aware MPI applications with both GCC and CUDA, and applications with NVIDIA HPC SDK with OpenACC, and is a stripped down version of a software stack that CSCS provides to the Swiss National Weather Service (MeteoSwiss) on Alps, specifically:
\begin{itemize}
    \item A GCC 11.3 compiler tool chain.
    \item An NVHPC 22.7 compiler tool chain.
    \item A GCC programming environment \lst{gcc-env} with CUDA-aware \craympich, OSU Benchmarks, OpenBLAS and CUDA 11.8.
    \item An NVHPC programming environment \lst{openac-env} with CUDA-aware \craympich, OSU Benchmarks and CUDA 11.8.
\end{itemize}

Spack stacks are generated from a descriptive YAML file recipe, composed of the following:
\begin{itemize}
    \item  \lstinline{config.yaml}:
        \lstinputlisting[language=yaml]{src/nv-recipe/config.yaml}
        Used to specify where the image will be installed/mounted (the CSCS default is \emph{/user-environment}), the version of \spack and optional configuration for a \spack build cache. Reproducible builds use a fixed version/commit of \spack, and rolling releases use the \lst{develop} branch of \spack.
    \item \lstinline{compilers.yaml} describes the compiler tool chains:
        \lstinputlisting[language=yaml]{src/nv-recipe/compilers.yaml}
        The bootstrap and GCC tool chains are mandatory, and it is allowed to specify more than one version of GCC.
        The LLVM tool chain is optional, with support for installing multiple versions of the NVIDIA HPC-SDK and LLVM/Clang.
    \item \lst{environment.yaml} describes the software packages;
        \lstinputlisting[language=yaml]{src/nv-recipe/environments.yaml}
        The software packages are configured as environments, each with built using the compiler tool chains built previously, and configured with a (optional) single implementation of \craympich, that can optionally be configured for CUDA or ROCM support.
    \item \lst{packages.yaml} and \lst{modules.yaml}
        for making \spack use packages installed on the system, and generating modules files respectively.
        These follow the YAML specifications for the \spack configuration files with the same names.
    \item \lst{repo}
        is an optional path containing a \href{https://spack.readthedocs.io/en/latest/repositories.html}{\spack repository}, for overriding \spack's implementations or providing support for new packages.
\end{itemize}

Under the hood, the software in the stack is built in a set of \spack environments.
For the example stack, there are five environments, illustrated in~\fig{fig:env-dag}.

\begin{figure}[htp!]
    \begin{center}
    \input{images/env-dag.tex}
    \end{center}
    \caption{The dependency graph between the \spack environments that are built to provide a \spack stack that provides GCC 11 and NVHPC 22.7 tool chains, and two environments built with the compilers. The bootstrap compiler is built first using the system GCC 7.5. Then GCC 11 tool chain, followed by the NVHPC tool chain which requires GCC 11. Finally two environments are built: gcc-env with GCC 11, and openacc-env which contains packages built with both GCC 11 and NVHPC 22.7.}
    \label{fig:env-dag}
\end{figure}

The Spack stack requires external packages installed on the base node image, that implement architecture-specific features and support, and can vary between vClusters and over time.
For example, the location of libfabric moves every time the installed version of libfabric is upgraded.
A set of ``cluster configurations'' for each of the vClusters on Alps is maintained separately from the recipes in the \stackinator tool:
\begin{itemize}
    \item A \spack \lst{compilers.yaml} file that specifies the default GCC compiler tool chain on the vCluster (GCC 7.5 at the time of writing), used to bootstrap the \spack stack build.
    \item A \spack \lst{packages.yaml} file that specifies externally installed software packages that Spack should never build, including:
    \begin{itemize}
        \item \lst{libfabric}: the libfabric library installed in \lst{/opt/cray/libfabric/} with CXI provider.
        \item \lst{slurm}: vClusters can have different versions of SLURM installed: at the time of writing versions 20.11.9 and 22.05.2 are used.
        \item \lst{xpmemm} and \lst{rdma-core}: required by some communication libraries.
    \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stack Configuration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\stackinator follows the familiar configure-build-install workflow used to install software.
It provides a CLI tool \lstinline{stack-config}, that takes a generic recipe that can be built on any (vCluster, mount point) combination, and generates a build path that contains a Makefile, \spack environment descriptions, and a copy of \spack used to build the stack.

If the recipe that describes the example environment in~\fig{fig:env-dag} is in the path \lstinline{~/recipes/nvidia}, \lstinline{stack-config} can be used to generate a build configuration with the following command:
\begin{lstlisting}
> stack-config -r ~/recipes/nvidia \
             -b /dev/shm/nvidia-build \
             -s hohgant
\end{lstlisting}
where the generated build path is in \lstinline{/dev/shm/nvidia-build} and the build is configured for the vCluster Hohgant.
A simplified version of the build directory structure is illustrated in~\fig{fig:build-tree}.

\begin{figure}[htp!]
    \input{src/build-path.tex}
    \caption{Structure of the build path generated by the \stackinator tool. The files marked in blue are generated by \spack during the build process in each of the five environment paths, and are only shown in the bootstrap path for brevity.}
    \label{fig:build-tree}
\end{figure}

The environment is built using the top-level Makefile, which executes the following steps:
\begin{enumerate}
    \item (*) configure build cache
    \item Call \lst{compilers/Makefile}:
    \begin{enumerate}
        \item Concretize bootstrap
        \item build bootstrap
        \item concretize GCC
        \item build GCC
        \item concretize LLVM (NVHPC)
        \item build LLVM (NVHPC)
    \end{enumerate}
    \item call \lst{environments/Makefile}:
    \begin{enumerate}
        \item concretize gcc-env and nvhpc-env concurrently
        \item build gcc-env and nvhpc-env concurrently
    \end{enumerate}
    \item generate \lst{store/config}
    \item (*) generate \lst{store/modules}
    \item generate SquashFS image of \lst{store}
\end{enumerate}

The \lstinline{compilers.yaml}, \lstinline{packages.yaml} and \lstinline{Makefile} for each of the \spack environments are generated in the Makefile using \spack. The commands used generate the respective files required to build the "gcc-env" are summarised:
\begin{itemize}
    \item \lst{gcc-env/compilers.yaml}:
        \begin{lstlisting}
> gcc_prefix= spack -e ../compilers/gcc \
        find --format '{prefix}' gcc@11
> spack compiler find --scope=user \
        $(compiler_bin_dirs $gcc_prefix) \end{lstlisting}
        Where the function \lst{compiler_bin_dirs} is a function that returns \lst{bin} path of the compiler, omitted for brevity.

    \item \lst{gcc-env/packages.yaml}:
        \begin{lstlisting}
> spack external find --not-buildable --scope=user  perl diffutils gettext \end{lstlisting}

    \item \lst{gcc-env/Makefile}:
        \begin{lstlisting}
> spack -e gcc-env/ concretize -f
> spack -e gcc-env/ env depfile \
        -o gcc-env/Makefile \end{lstlisting}
        First, the environment is concretized (\spack generates the full set of packages and their dependencies for the environment), then a Makefile that facilitates building packages in parallel is generated.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Building a Stack}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Make is run with an empty environment with \lst{env}, to improve reproducibility by eliminating the effect of environment variables on the build:
\begin{lstlisting}
> cd /dev/shm/nvidia-build
> env --ignore-environment \
    PATH=/usr/bin:/bin:`pwd`/spack/bin \
    make store.squashfs -j32
\end{lstlisting}
Two artifacts are generated by the build:
\begin{itemize}
    \item \lst{store}: a path contains the full PE, ready to be copied to its final location.
    \item \lst{store.squashfs} path compressed in a SquashFS image, which can be mounted at the mount point.
\end{itemize}
The deployment of the SquashFS images is covered in~\sect{sec:deployment}.

The build process uses the Bubblewrap tool when running all spack commands to mount the following locations:
\begin{itemize}
    \item \lst{/dev/shm/nvidia-build/store} is mounted at the final location for the \spack stack. Thus the stack is built in a location where the user has write permissions, and allows faster in-memory builds (see~\sect{sec:faster-builds}).
    \item \lst{/dev/null} is mounted at HOME to improve reproducibility by ignoring any \spack configuration in HOME.
    \item \lst{/dev/shm/nvidia-build/tmp} is mounted at \lst{/tmp} to retain all \spack logs.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MPI}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Open source MPI distributions -- namely OpenMPI, MVAPICH2 and MPICH -- are actively developing support for Slingshot 11 with libfabric.
However, at the time of writing the only MPI with robust Slingshot 11 support is the \craympich bundled with the CPE, for which source code is not available.

One of the objectives of this work is to provide software stacks without installing CPE.
In order to provide \craympich, we develop a process for repackaging the compiler wrappers, library, headers and dependencies like PMI that can be installed as a \spack binary package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Step 1: extract and repackage RPMs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% package names for nvhpc (rpm -qi *rpm | grep Name)
% Name        : cray-mpich-8.1.24-gtl
% Name        : cray-mpich-8.1.24-nvidia207
% Name        : cray-pmi-6.1.9
% Name        : cray-pmi-devel-6.1.9

% package names for gnu
% Name        : cray-mpich-8.1.24-gnu91
% Name        : cray-mpich-8.1.24-gtl
% Name        : cray-pmi-6.1.9
% Name        : cray-pmi-devel-6.1.9

Expand the following RPMs from the CPE distribution of the following packages:
\begin{enumerate}
    \item \lst{cray-mpich-8.1.18-gnu91}
    \begin{itemize}
        \item For the NVIDIA HPC SDK use \lst{cray-mpich-8.1.18-nvidia207}.
    \end{itemize}
    \item \lst{cray-mpich-8.1.18-gtl}
    \begin{itemize}
        \item The GTL (GPU Transport Layer) libraries that implement GPU-aware communication for NVIDIA and AMD GPUs.
    \end{itemize}
    % from cpe-23.02 there is also cray-mpi-devel
    \item \lst{cray-pmi-[version]}
\end{enumerate}

We can't provide a single binary distribution for all compilers, because the Fortran modules in the \lst{include} path are compiler specific.

Extract headers, binaries and libraries from the RPMs, and gather them in a directory with structure illustrated in~\fig{fig:mpich-tree}.

\begin{figure}[htp!]
    \input{src/cray-mpich-tree-nosym.txt}
    \caption{ Note that for brevity symlinked library files are removed, and wildcards are used to describe headers.}
    \label{fig:mpich-tree}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Step 2: Patch MPI wrappers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As opposed to the CPE, which provides the \lst{CC}, \lst{cc} and \lst{ftn} binary compiler wrappers, our package uses the MPI compiler wrapper scripts that are installed in locations like \lst{/opt/cray/pe/mpich/8.1.21/ofi/gnu/9.1/bin}.

By default, these wrappers use environment variables set by CPE modules to select the compiler and link architecture-specific libraries.
We modify each of the wrappers by parameterizing them on three parameters -- \lst{@CC@}, \lst{@@PREFIX@@} and \lst{@@GTL_LIBRARY@@} --
that are set by \spack when they are installed:
\begin{itemize}
    \item hard code the full path to the wrapped compiler;
    \item set paths \lst{prefix}, \lst{includedir}, to the cray-mpich \spack installation path;
    \item explicitly link \lst{-lmpi_gtl_cuda} \lst{-lmpi_gtl_hsa} when \craympich is built with the \lst{+cuda} or \lst{+rocm} variants respectively.
    % \item \begin{itemize}
    %     %\item this fixes the common runtime error ``MPIDI\_CRAY\_init: GPU\_SUPPORT\_ENABLED'' is requested, but GTL library is not linked.
    % \end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Step 3: Create patch binary \spack package}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The tar balls are stored in a \href{https://jfrog.com/artifactory/}{JFrog Artifactory} -- a self-hosted artifact store accessible only on the CSCS network.
The \spack package directly downloads from the artifactory, so it can only be run on CSCS systems.

\href{https://github.com/eth-cscs/stackinator/blob/master/stackinator/repo/packages/cray-mpich/package.py}{\lst{/stackinator/repo/packages/cray-mpich/package.py}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Efficient Stack Builds}
\label{sec:faster-builds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Using Spack to build a full software stack -- with multiple compilers, libraries and tools -- is time and resource consuming.
A simple stack based on GCC that provides Python, \craympich and CUDA, will take in the order of half an hour to build, and environments for the ROCM GPU stack take over two hours to build from scratch on a 64-core Epyc CPU.

It is important to reduce build times where possible, so that maintainers of stacks can iterate and test combinations of packages, and for timely execution of CI/CD pipelines for deploying stacks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Parallelise the build}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As illustrated in~\fig{fig:env-dag}, building a \spack stack involves building a \spack environments with a DAG of dependencies dictating the order of environment concretization/installation.
In turn, installing an environment builds individual packages, which have their own DAG of dependencies.

The Makefiles generated by the \stackinator define the dependencies between environments -- facilitating the concurrent concretization and installation of independent environments. The Makefile used to build each environment is generated using the
\href{https://spack.readthedocs.io/en/latest/environments.html#generating-depfiles-from-environments}{dependency file generation} feature of \spack, make it possible to build multiple packages in parallel.
A single jobserver is used to parallelise the concurrent environment and package builds.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Perform builds in memory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \stackinator tool supports building software stacks for installation at mount points where the build process does not have write permissions, e.g. the default \lst{/user-environment} path is read-only.
The tool uses \href{https://github.com/containers/bubblewrap}{bubblewrap} to mount the build path.
This can be used to optimise build times by creating the build path in memory, for example in \lst{/dev/shm/$USER/$PROJECTNAME}, so that all of the dependencies are built and stored in memory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Cache previously built packages}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The most effective way to reduce build times is to cache built packages.
\spack provides \href{https://spack.readthedocs.io/en/latest/binary_caches.html}{build caches}, which facilitate pushing and pulling pre-built packages to S3 buckets or a file system.
\stackinator recipes can include optional \lst{mirrors.yaml} file and a private key, to enable build caches.
Providing the location of a build cache with \lst{mirrors.yaml} will enable pulling packages from the cache, and if a key is provided \stackinator will also push all packages to the build cache as they are built.


%%% Local Variables:
%%% TeX-master: "paper"
%%% End:
